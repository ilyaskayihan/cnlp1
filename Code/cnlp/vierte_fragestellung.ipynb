{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T23:16:54.340262Z",
     "start_time": "2025-06-09T23:16:26.939343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 f√ºr Produkt 203240...\n",
      "‚úÖ Ergebnisse gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Client setup\n",
    "client = OpenAI(\n",
    "    api_key=\"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\", \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Temperaturwerte, die du vergleichen willst\n",
    "TEMPERATURES = [0.0, 0.7, 1.2]\n",
    "\n",
    "# GPT-4 Modell\n",
    "MODEL_ID = \"openai/gpt-4\"\n",
    "\n",
    "# Funktion zum Erstellen der Produktbeschreibung\n",
    "def create_product_description(group):\n",
    "    lines = []\n",
    "    for _, row in group.iterrows():\n",
    "        line = f\"{row['PropertyDefinition']}: {row['PropertyValue']}\"\n",
    "        if pd.notnull(row['UnitName']):\n",
    "            line += f\" {row['UnitName']}\"\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Prompt-Generierung (neue Halluzinationsfrage)\n",
    "def generate_prompt_from_group(group):\n",
    "    product_text = create_product_description(group)\n",
    "    return f\"\"\"\n",
    "Hier ist die Beschreibung eines Produkts. Antworte pr√§zise auf folgende Frage:\n",
    "\n",
    "‚ÄûWelche besonderen technischen Innovationen bietet dieses Produkt?‚Äú\n",
    "\n",
    "Antworte in maximal 3 S√§tzen und gib bitte nur konkrete Merkmale an, die sich aus der Produktbeschreibung ableiten lassen.\n",
    "\n",
    "{product_text}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt an GPT-4 senden mit konfigurierbarer Temperatur\n",
    "def run_prompt(prompt, temperature):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Main-Funktion\n",
    "def main():\n",
    "    df = pd.read_parquet(\"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/241212_processedproperties_fhnw.parquet\")\n",
    "    grouped = df.groupby(\"ProductId\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for product_id, group in list(grouped)[:2]:  # Anzahl Produkte begrenzen\n",
    "        prompt = generate_prompt_from_group(group)\n",
    "        row_result = {\n",
    "            \"ProductId\": product_id,\n",
    "            \"Prompt\": prompt\n",
    "        }\n",
    "\n",
    "        for temp in TEMPERATURES:\n",
    "            try:\n",
    "                print(f\"‚è≥ Anfrage an GPT-4 mit temperature={temp} f√ºr Produkt {product_id}...\")\n",
    "                output = run_prompt(prompt, temp)\n",
    "                row_result[f\"output_temp_{temp}\"] = output\n",
    "                time.sleep(1.5)\n",
    "            except Exception as e:\n",
    "                row_result[f\"output_temp_{temp}\"] = f\"Fehler: {str(e)}\"\n",
    "\n",
    "        results.append(row_result)\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    pd.DataFrame(results).to_csv(\"produkt_halluzinationen_sampling.csv\", index=False)\n",
    "    print(\"‚úÖ Ergebnisse gespeichert.\")\n",
    "\n",
    "# Skript ausf√ºhren\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spaltennamen in CSV: ['ProductId', 'Prompt', 'output_temp_0.0', 'output_temp_0.7', 'output_temp_1.2']\n",
      "üîç Pr√ºfe Produkt 200440 mit output_temp_0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit output_temp_0.7...\n",
      "üîç Pr√ºfe Produkt 200440 mit output_temp_1.2...\n",
      "üîç Pr√ºfe Produkt 203240 mit output_temp_0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit output_temp_0.7...\n",
      "üîç Pr√ºfe Produkt 203240 mit output_temp_1.2...\n",
      "‚úÖ Fertig! Ergebnisse in 'gpt4_factcheck_bewertung.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# üîç GPT-4 Fact-Check Prompt\n",
    "def generate_factcheck_prompt(product_facts, model_output):\n",
    "    return f\"\"\"\n",
    "Du bist ein Experte f√ºr Faktenpr√ºfung im E-Commerce.\n",
    "\n",
    "### Aufgabe:\n",
    "Bewerte die folgende Produktzusammenfassung auf ihre **Faktentreue**. Verwende ausschlie√ülich die unten angegebenen Produktinformationen.\n",
    "\n",
    "Gib nur eine der folgenden Kategorien als Antwort zur√ºck:\n",
    "1. Faktisch korrekt\n",
    "2. Teilweise korrekt\n",
    "3. Halluzination\n",
    "\n",
    "### Produktinformation:\n",
    "{product_facts}\n",
    "\n",
    "### Zusammenfassung:\n",
    "{model_output}\n",
    "\n",
    "### Bewertung (nur eine Zeile mit der passenden Kategorie):\n",
    "\"\"\"\n",
    "\n",
    "# ‚úÖ GPT-4 Abfragefunktion\n",
    "def run_gpt4_factcheck(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# üîÑ Main\n",
    "def main():\n",
    "    # CSV laden\n",
    "    df = pd.read_csv(\n",
    "        \"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/produkt_halluzinationen_sampling.csv\",\n",
    "        sep=\";\",\n",
    "        quotechar='\"',\n",
    "        on_bad_lines='skip',\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"Spaltennamen in CSV:\", df.columns.tolist())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_id = row[\"ProductId\"]\n",
    "        prompt_text = row[\"Prompt\"]\n",
    "\n",
    "        for temp_col in [\"output_temp_0.0\", \"output_temp_0.7\", \"output_temp_1.2\"]:\n",
    "            model_output = row[temp_col]\n",
    "            full_prompt = generate_factcheck_prompt(prompt_text, model_output)\n",
    "\n",
    "            try:\n",
    "                print(f\"üîç Pr√ºfe Produkt {product_id} mit {temp_col}...\")\n",
    "                evaluation = run_gpt4_factcheck(full_prompt)\n",
    "            except Exception as e:\n",
    "                evaluation = f\"Fehler: {e}\"\n",
    "\n",
    "            results.append({\n",
    "                \"ProductId\": product_id,\n",
    "                \"Prompt\": prompt_text,\n",
    "                \"Temperature_Output\": temp_col,\n",
    "                \"Model_Output\": model_output,\n",
    "                \"GPT-4_Faktenbewertung\": evaluation\n",
    "            })\n",
    "\n",
    "            time.sleep(1.5)  # Rate Limit vermeiden\n",
    "\n",
    "    # Ergebnis speichern\n",
    "    pd.DataFrame(results).to_csv(\"gpt4_factcheck_bewertung.csv\", index=False)\n",
    "    print(\"‚úÖ Fertig! Ergebnisse in 'gpt4_factcheck_bewertung.csv' gespeichert.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-09T23:37:04.542411Z",
     "start_time": "2025-06-09T23:36:47.920397Z"
    }
   },
   "id": "48a1550254aae774"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jetzt machen wir einen anderen test weil alles faktisch korrekt war."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "402e138bf8822437"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 f√ºr Produkt 203240...\n",
      "‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_sampling_extrem.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Client setup\n",
    "client = OpenAI(\n",
    "    api_key=\"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\", \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Temperaturwerte, die du vergleichen willst\n",
    "TEMPERATURES = [0.0, 0.7, 1.2]\n",
    "\n",
    "# GPT-4 Modell\n",
    "MODEL_ID = \"openai/gpt-4\"\n",
    "\n",
    "# Funktion zum Erstellen der Produktbeschreibung\n",
    "def create_product_description(group):\n",
    "    lines = []\n",
    "    for _, row in group.iterrows():\n",
    "        line = f\"{row['PropertyDefinition']}: {row['PropertyValue']}\"\n",
    "        if pd.notnull(row['UnitName']):\n",
    "            line += f\" {row['UnitName']}\"\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Prompt-Generierung ‚Äì EXTREM herausfordernde Frage, um Halluzinationen zu provozieren\n",
    "def generate_prompt_from_group(group):\n",
    "    product_text = create_product_description(group)\n",
    "    return f\"\"\"\n",
    "Du bist ein sehr kritischer Produktexperte.\n",
    "\n",
    "Hier ist die Beschreibung eines Produkts:\n",
    "{product_text}\n",
    "\n",
    "Frage: Welche besonderen und innovativen technischen Eigenschaften oder Funktionen hat dieses Produkt? Bitte antworte m√∂glichst detailliert in maximal 3 S√§tzen.\n",
    "\n",
    "WICHTIG: Wenn keine eindeutigen Informationen im Text vorhanden sind, antworte nicht mit erfundenen Details, sondern sage, dass keine weiteren Informationen vorliegen.\n",
    "\n",
    "ANTWORT:\n",
    "\"\"\"\n",
    "\n",
    "# Prompt an GPT-4 senden mit konfigurierbarer Temperatur\n",
    "def run_prompt(prompt, temperature):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Main-Funktion\n",
    "def main():\n",
    "    df = pd.read_parquet(\"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/241212_processedproperties_fhnw.parquet\")\n",
    "    grouped = df.groupby(\"ProductId\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for product_id, group in list(grouped)[:2]:  # Anzahl Produkte begrenzen\n",
    "        prompt = generate_prompt_from_group(group)\n",
    "        row_result = {\n",
    "            \"ProductId\": product_id,\n",
    "            \"Prompt\": prompt\n",
    "        }\n",
    "\n",
    "        for temp in TEMPERATURES:\n",
    "            try:\n",
    "                print(f\"‚è≥ Anfrage an GPT-4 mit temperature={temp} f√ºr Produkt {product_id}...\")\n",
    "                output = run_prompt(prompt, temp)\n",
    "                row_result[f\"output_temp_{temp}\"] = output\n",
    "                time.sleep(1.5)\n",
    "            except Exception as e:\n",
    "                row_result[f\"output_temp_{temp}\"] = f\"Fehler: {str(e)}\"\n",
    "\n",
    "        results.append(row_result)\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    pd.DataFrame(results).to_csv(\"produkt_halluzinationen_sampling_extrem.csv\", index=False)\n",
    "    print(\"‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_sampling_extrem.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-09T23:41:31.805003Z",
     "start_time": "2025-06-09T23:40:59.694218Z"
    }
   },
   "id": "3993143db09bd5c5"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spaltennamen in CSV: ['ProductId', 'Prompt', 'output_temp_0.0', 'output_temp_0.7', 'output_temp_1.2']\n",
      "üîç Pr√ºfe Produkt 200440 mit output_temp_0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit output_temp_0.7...\n",
      "üîç Pr√ºfe Produkt 200440 mit output_temp_1.2...\n",
      "üîç Pr√ºfe Produkt 203240 mit output_temp_0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit output_temp_0.7...\n",
      "üîç Pr√ºfe Produkt 203240 mit output_temp_1.2...\n",
      "‚úÖ Fertig! Ergebnisse in 'gpt4_factcheck_bewertung_extrem.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "def generate_factcheck_prompt(product_facts, model_output):\n",
    "    return f\"\"\"\n",
    "Du bist ein strenger Faktenpr√ºfer im Bereich E-Commerce. Deine Aufgabe ist es, jede Aussage in der Produktzusammenfassung **sehr genau** mit den gegebenen Produktinformationen zu vergleichen.\n",
    "\n",
    "Bitte kategorisiere die Zusammenfassung in eine der folgenden Kategorien:\n",
    "\n",
    "1. **Faktisch korrekt** ‚Äì Alle Aussagen stimmen exakt mit den Produktinformationen √ºberein.\n",
    "2. **Teilweise korrekt** ‚Äì Einige Aussagen sind korrekt, aber es gibt auch Fehler oder √úbertreibungen.\n",
    "3. **Halluzination** ‚Äì Die Zusammenfassung enth√§lt falsche, erfundene oder nicht belegbare Informationen.\n",
    "\n",
    "Wichtig: Sei kritisch und z√∂gere nicht, 'Halluzination' zu vergeben, wenn eine Behauptung nicht klar durch die Produktinformationen gedeckt ist.\n",
    "\n",
    "---\n",
    "\n",
    "### Produktinformation:\n",
    "{product_facts}\n",
    "\n",
    "### Zusammenfassung:\n",
    "{model_output}\n",
    "\n",
    "### Deine Bewertung (nur eine Kategorie als Antwort, keine Erkl√§rungen):\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ‚úÖ GPT-4 Abfragefunktion\n",
    "def run_gpt4_factcheck(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# üîÑ Main\n",
    "def main():\n",
    "    # CSV laden (mit den extremen Outputs)\n",
    "    df = pd.read_csv(\n",
    "        \"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/produkt_halluzinationen_sampling_extrem.csv\",\n",
    "        sep=\",\",  # CSV aus vorherigem Code speichert mit Komma als Trennzeichen\n",
    "        quotechar='\"',\n",
    "        on_bad_lines='skip',\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"Spaltennamen in CSV:\", df.columns.tolist())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_id = row[\"ProductId\"]\n",
    "        prompt_text = row[\"Prompt\"]\n",
    "\n",
    "        for temp_col in [\"output_temp_0.0\", \"output_temp_0.7\", \"output_temp_1.2\"]:\n",
    "            model_output = row[temp_col]\n",
    "            full_prompt = generate_factcheck_prompt(prompt_text, model_output)\n",
    "\n",
    "            try:\n",
    "                print(f\"üîç Pr√ºfe Produkt {product_id} mit {temp_col}...\")\n",
    "                evaluation = run_gpt4_factcheck(full_prompt)\n",
    "            except Exception as e:\n",
    "                evaluation = f\"Fehler: {e}\"\n",
    "\n",
    "            results.append({\n",
    "                \"ProductId\": product_id,\n",
    "                \"Prompt\": prompt_text,\n",
    "                \"Temperature_Output\": temp_col,\n",
    "                \"Model_Output\": model_output,\n",
    "                \"GPT-4_Faktenbewertung\": evaluation\n",
    "            })\n",
    "\n",
    "            time.sleep(1.5)  # Rate Limit vermeiden\n",
    "\n",
    "    # Ergebnis speichern\n",
    "    pd.DataFrame(results).to_csv(\"gpt4_factcheck_bewertung_extrem.csv\", index=False)\n",
    "    print(\"‚úÖ Fertig! Ergebnisse in 'gpt4_factcheck_bewertung_extrem.csv' gespeichert.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-09T23:46:43.834739Z",
     "start_time": "2025-06-09T23:46:30.030078Z"
    }
   },
   "id": "c5ab1fe511b548ce"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 und top_p=0.7 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 und top_p=0.85 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 und top_p=1.0 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 und top_p=0.7 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 und top_p=0.85 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 und top_p=1.0 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 und top_p=0.7 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 und top_p=0.85 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 und top_p=1.0 f√ºr Produkt 200440...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 und top_p=0.7 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 und top_p=0.85 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.0 und top_p=1.0 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 und top_p=0.7 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 und top_p=0.85 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=0.7 und top_p=1.0 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 und top_p=0.7 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 und top_p=0.85 f√ºr Produkt 203240...\n",
      "‚è≥ Anfrage an GPT-4 mit temperature=1.2 und top_p=1.0 f√ºr Produkt 203240...\n",
      "‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_sampling_topptest.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Client setup\n",
    "client = OpenAI(\n",
    "    api_key=\"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\", \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Temperatur- und Top-p Werte, die du vergleichen willst\n",
    "TEMPERATURES = [0.0, 0.7, 1.2]\n",
    "TOP_P_VALUES = [0.7, 0.85, 1.0]\n",
    "\n",
    "# GPT-4 Modell\n",
    "MODEL_ID = \"openai/gpt-4\"\n",
    "\n",
    "# Funktion zum Erstellen der Produktbeschreibung\n",
    "def create_product_description(group):\n",
    "    lines = []\n",
    "    for _, row in group.iterrows():\n",
    "        line = f\"{row['PropertyDefinition']}: {row['PropertyValue']}\"\n",
    "        if pd.notnull(row['UnitName']):\n",
    "            line += f\" {row['UnitName']}\"\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Prompt-Generierung ‚Äì EXTREM herausfordernde Frage, um Halluzinationen zu provozieren\n",
    "def generate_prompt_from_group(group):\n",
    "    product_text = create_product_description(group)\n",
    "    return f\"\"\"\n",
    "Du bist ein sehr kritischer Produktexperte.\n",
    "\n",
    "Hier ist die Beschreibung eines Produkts:\n",
    "{product_text}\n",
    "\n",
    "Frage: Welche besonderen und innovativen technischen Eigenschaften oder Funktionen hat dieses Produkt? Bitte antworte m√∂glichst detailliert in maximal 3 S√§tzen.\n",
    "\n",
    "WICHTIG: Wenn keine eindeutigen Informationen im Text vorhanden sind, antworte nicht mit erfundenen Details, sondern sage, dass keine weiteren Informationen vorliegen.\n",
    "\n",
    "ANTWORT:\n",
    "\"\"\"\n",
    "\n",
    "# Prompt an GPT-4 senden mit konfigurierbarer Temperatur und top_p\n",
    "def run_prompt(prompt, temperature, top_p):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Main-Funktion\n",
    "def main():\n",
    "    df = pd.read_parquet(\"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/241212_processedproperties_fhnw.parquet\")\n",
    "    grouped = df.groupby(\"ProductId\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Nur zwei Produkte zum Testen, kannst du erh√∂hen\n",
    "    for product_id, group in list(grouped)[:2]:  \n",
    "        prompt = generate_prompt_from_group(group)\n",
    "        row_result = {\n",
    "            \"ProductId\": product_id,\n",
    "            \"Prompt\": prompt\n",
    "        }\n",
    "\n",
    "        for temp in TEMPERATURES:\n",
    "            for top_p in TOP_P_VALUES:\n",
    "                try:\n",
    "                    print(f\"‚è≥ Anfrage an GPT-4 mit temperature={temp} und top_p={top_p} f√ºr Produkt {product_id}...\")\n",
    "                    output = run_prompt(prompt, temp, top_p)\n",
    "                    key = f\"output_temp_{temp}_topp_{top_p}\"\n",
    "                    row_result[key] = output\n",
    "                    time.sleep(1.5)\n",
    "                except Exception as e:\n",
    "                    key = f\"output_temp_{temp}_topp_{top_p}\"\n",
    "                    row_result[key] = f\"Fehler: {str(e)}\"\n",
    "\n",
    "        results.append(row_result)\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    pd.DataFrame(results).to_csv(\"produkt_halluzinationen_sampling_topptest.csv\", index=False)\n",
    "    print(\"‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_sampling_topptest.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-09T23:51:29.158890Z",
     "start_time": "2025-06-09T23:49:52.412908Z"
    }
   },
   "id": "3dfb633e4fb08a0e"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.9, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.9, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.9, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.9, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.8, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.8, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.8, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.9, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.9, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.9, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=0.9, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=1.0, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=1.0, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.7, top_p=1.0, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.8, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.8, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.8, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.9, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.9, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.9, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=0.9, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=1.0, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=1.0, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=1.2, top_p=1.0, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.9, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.9, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.9, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.9, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.8, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.8, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.8, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.9, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.9, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.9, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=0.9, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=1.0, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=1.0, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.7, top_p=1.0, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.8, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.8, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.8, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.9, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.9, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.9, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=0.9, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=1.0, freq_pen=0.0, pres_pen=0.5 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=1.0, freq_pen=0.5, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=1.2, top_p=1.0, freq_pen=0.5, pres_pen=0.5 ...\n",
      "‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_sampling_advanced.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Client setup\n",
    "client = OpenAI(\n",
    "    api_key=\"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\", \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Sampling-Parameter\n",
    "TEMPERATURES = [0.0, 0.7, 1.2]\n",
    "TOP_P_VALUES = [0.8, 0.9, 1.0]\n",
    "FREQUENCY_PENALTIES = [0.0, 0.5]\n",
    "PRESENCE_PENALTIES = [0.0, 0.5]\n",
    "\n",
    "MODEL_ID = \"openai/gpt-4\"\n",
    "\n",
    "def create_product_description(group):\n",
    "    lines = []\n",
    "    for _, row in group.iterrows():\n",
    "        line = f\"{row['PropertyDefinition']}: {row['PropertyValue']}\"\n",
    "        if pd.notnull(row['UnitName']):\n",
    "            line += f\" {row['UnitName']}\"\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def generate_prompt_from_group(group):\n",
    "    product_text = create_product_description(group)\n",
    "    return f\"\"\"\n",
    "Du bist ein sehr kritischer Produktexperte.\n",
    "\n",
    "Hier ist die Beschreibung eines Produkts:\n",
    "{product_text}\n",
    "\n",
    "Frage: Welche besonderen und innovativen technischen Eigenschaften oder Funktionen hat dieses Produkt? Bitte antworte m√∂glichst detailliert in maximal 3 S√§tzen.\n",
    "\n",
    "WICHTIG: Wenn keine eindeutigen Informationen im Text vorhanden sind, antworte nicht mit erfundenen Details, sondern sage, dass keine weiteren Informationen vorliegen.\n",
    "\n",
    "ANTWORT:\n",
    "\"\"\"\n",
    "\n",
    "def run_prompt(prompt, temperature, top_p, frequency_penalty, presence_penalty):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    df = pd.read_parquet(\"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/241212_processedproperties_fhnw.parquet\")\n",
    "    grouped = df.groupby(\"ProductId\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for product_id, group in list(grouped)[:2]:  # Limit f√ºr Test\n",
    "        prompt = generate_prompt_from_group(group)\n",
    "        row_result = {\n",
    "            \"ProductId\": product_id,\n",
    "            \"Prompt\": prompt\n",
    "        }\n",
    "\n",
    "        for temp in TEMPERATURES:\n",
    "            for top_p in TOP_P_VALUES:\n",
    "                for freq_penalty in FREQUENCY_PENALTIES:\n",
    "                    for pres_penalty in PRESENCE_PENALTIES:\n",
    "                        try:\n",
    "                            print(f\"‚è≥ Anfrage f√ºr Produkt {product_id} mit temp={temp}, top_p={top_p}, freq_pen={freq_penalty}, pres_pen={pres_penalty} ...\")\n",
    "                            output = run_prompt(prompt, temp, top_p, freq_penalty, pres_penalty)\n",
    "                            key = f\"output_t{temp}_p{top_p}_f{freq_penalty}_pr{pres_penalty}\"\n",
    "                            row_result[key] = output\n",
    "                            time.sleep(1.5)\n",
    "                        except Exception as e:\n",
    "                            key = f\"output_t{temp}_p{top_p}_f{freq_penalty}_pr{pres_penalty}\"\n",
    "                            row_result[key] = f\"Fehler: {str(e)}\"\n",
    "\n",
    "        results.append(row_result)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(\"produkt_halluzinationen_sampling_advanced.csv\", index=False)\n",
    "    print(\"‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_sampling_advanced.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T00:01:39.873598Z",
     "start_time": "2025-06-09T23:55:22.891060Z"
    }
   },
   "id": "3629a03647b672ce"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spaltennamen in CSV: ['ProductId', 'Prompt', 'output_t0.0_p0.8_f0.0_pr0.0', 'output_t0.0_p0.8_f0.0_pr0.5', 'output_t0.0_p0.8_f0.5_pr0.0', 'output_t0.0_p0.8_f0.5_pr0.5', 'output_t0.0_p0.9_f0.0_pr0.0', 'output_t0.0_p0.9_f0.0_pr0.5', 'output_t0.0_p0.9_f0.5_pr0.0', 'output_t0.0_p0.9_f0.5_pr0.5', 'output_t0.0_p1.0_f0.0_pr0.0', 'output_t0.0_p1.0_f0.0_pr0.5', 'output_t0.0_p1.0_f0.5_pr0.0', 'output_t0.0_p1.0_f0.5_pr0.5', 'output_t0.7_p0.8_f0.0_pr0.0', 'output_t0.7_p0.8_f0.0_pr0.5', 'output_t0.7_p0.8_f0.5_pr0.0', 'output_t0.7_p0.8_f0.5_pr0.5', 'output_t0.7_p0.9_f0.0_pr0.0', 'output_t0.7_p0.9_f0.0_pr0.5', 'output_t0.7_p0.9_f0.5_pr0.0', 'output_t0.7_p0.9_f0.5_pr0.5', 'output_t0.7_p1.0_f0.0_pr0.0', 'output_t0.7_p1.0_f0.0_pr0.5', 'output_t0.7_p1.0_f0.5_pr0.0', 'output_t0.7_p1.0_f0.5_pr0.5', 'output_t1.2_p0.8_f0.0_pr0.0', 'output_t1.2_p0.8_f0.0_pr0.5', 'output_t1.2_p0.8_f0.5_pr0.0', 'output_t1.2_p0.8_f0.5_pr0.5', 'output_t1.2_p0.9_f0.0_pr0.0', 'output_t1.2_p0.9_f0.0_pr0.5', 'output_t1.2_p0.9_f0.5_pr0.0', 'output_t1.2_p0.9_f0.5_pr0.5', 'output_t1.2_p1.0_f0.0_pr0.0', 'output_t1.2_p1.0_f0.0_pr0.5', 'output_t1.2_p1.0_f0.5_pr0.0', 'output_t1.2_p1.0_f0.5_pr0.5']\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.9_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.9_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.9_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.9_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.8_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.8_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.8_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.9_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.9_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.9_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p0.9_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p1.0_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p1.0_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.7_p1.0_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.8_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.8_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.8_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.9_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.9_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.9_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p0.9_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p1.0_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p1.0_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t1.2_p1.0_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.9_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.9_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.9_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.9_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.8_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.8_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.8_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.9_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.9_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.9_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p0.9_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p1.0_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p1.0_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.7_p1.0_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.8_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.8_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.8_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.9_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.9_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.9_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p0.9_f0.5_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p1.0_f0.0_pr0.5...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p1.0_f0.5_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t1.2_p1.0_f0.5_pr0.5...\n",
      "‚úÖ Faktenbewertungen gespeichert in 'gpt4_factcheck_bewertung_advanced.csv'.\n",
      "‚è≥ Generiere Sampling-Ranking mit LLM...\n",
      "\n",
      "--- Sampling Ranking ---\n",
      "\n",
      "Da alle Sampling-Setups die gleiche Bewertung \"Faktisch korrekt\" haben und keine Halluzinationen oder teilweise korrekten Bewertungen vorhanden sind, k√∂nnen sie alle als gleich gut angesehen werden. Daher ist das Ranking gleichwertig f√ºr alle Setups.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "def generate_factcheck_prompt(product_facts, model_output):\n",
    "    return f\"\"\"\n",
    "Du bist ein strenger Faktenpr√ºfer im Bereich E-Commerce. Deine Aufgabe ist es, jede Aussage in der Produktzusammenfassung **sehr genau** mit den gegebenen Produktinformationen zu vergleichen.\n",
    "\n",
    "Bitte kategorisiere die Zusammenfassung in eine der folgenden Kategorien:\n",
    "\n",
    "1. **Faktisch korrekt** ‚Äì Alle Aussagen stimmen exakt mit den Produktinformationen √ºberein.\n",
    "2. **Teilweise korrekt** ‚Äì Einige Aussagen sind korrekt, aber es gibt auch Fehler oder √úbertreibungen.\n",
    "3. **Halluzination** ‚Äì Die Zusammenfassung enth√§lt falsche, erfundene oder nicht belegbare Informationen.\n",
    "\n",
    "Wichtig: Sei kritisch und z√∂gere nicht, 'Halluzination' zu vergeben, wenn eine Behauptung nicht klar durch die Produktinformationen gedeckt ist.\n",
    "\n",
    "---\n",
    "\n",
    "### Produktinformation:\n",
    "{product_facts}\n",
    "\n",
    "### Zusammenfassung:\n",
    "{model_output}\n",
    "\n",
    "### Deine Bewertung (nur eine Kategorie als Antwort, keine Erkl√§rungen):\n",
    "\"\"\"\n",
    "\n",
    "def run_gpt4_factcheck(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def generate_ranking_prompt(results):\n",
    "    # Simple Zusammenfassung der Bewertungen nach Sampling-Parameter\n",
    "    # Wir wollen wissen, welche Kombinationen (temperature, top_p, freq_penalty, pres_penalty) am wenigsten Halluzinationen produzieren\n",
    "    text = \"Bewerte folgendes Sampling-Setup-Ranking basierend auf der H√§ufigkeit von 'Faktisch korrekt', 'Teilweise korrekt' und 'Halluzination' Bewertungen. Gib eine Rangliste der besten Sampling-Parameter zur√ºck, die m√∂glichst wenige Halluzinationen enthalten und gute Faktentreue aufweisen.\\n\\n\"\n",
    "    for r in results:\n",
    "        text += f\"Sampling: {r['Sampling']}, Bewertung: {r['GPT-4_Faktenbewertung']}\\n\"\n",
    "    text += \"\\nBitte gib ein Ranking in folgender Form zur√ºck:\\n1. Sampling: ...\\n2. Sampling: ...\\n3. Sampling: ...\\n\\nNur das Ranking ohne weitere Erkl√§rungen.\"\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(\n",
    "        \"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/produkt_halluzinationen_sampling_advanced.csv\",\n",
    "        sep=\",\",\n",
    "        quotechar='\"',\n",
    "        on_bad_lines='skip',\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(\"Spaltennamen in CSV:\", df.columns.tolist())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Filtere alle Spalten mit Output (beginnt mit \"output_\")\n",
    "    output_columns = [col for col in df.columns if col.startswith(\"output_\")]\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_id = row[\"ProductId\"]\n",
    "        prompt_text = row[\"Prompt\"]\n",
    "\n",
    "        for col in output_columns:\n",
    "            model_output = row[col]\n",
    "            full_prompt = generate_factcheck_prompt(prompt_text, model_output)\n",
    "\n",
    "            try:\n",
    "                print(f\"üîç Pr√ºfe Produkt {product_id} mit Sampling {col}...\")\n",
    "                evaluation = run_gpt4_factcheck(full_prompt)\n",
    "            except Exception as e:\n",
    "                evaluation = f\"Fehler: {e}\"\n",
    "\n",
    "            results.append({\n",
    "                \"ProductId\": product_id,\n",
    "                \"Prompt\": prompt_text,\n",
    "                \"Sampling\": col,\n",
    "                \"Model_Output\": model_output,\n",
    "                \"GPT-4_Faktenbewertung\": evaluation\n",
    "            })\n",
    "\n",
    "            time.sleep(1.5)\n",
    "\n",
    "    # Ergebnis speichern\n",
    "    pd.DataFrame(results).to_csv(\"gpt4_factcheck_bewertung_advanced.csv\", index=False)\n",
    "    print(\"‚úÖ Faktenbewertungen gespeichert in 'gpt4_factcheck_bewertung_advanced.csv'.\")\n",
    "\n",
    "    # Ranking prompt an LLM senden\n",
    "    ranking_prompt = generate_ranking_prompt(results)\n",
    "    print(\"‚è≥ Generiere Sampling-Ranking mit LLM...\")\n",
    "    ranking_response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": ranking_prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    ranking = ranking_response.choices[0].message.content.strip()\n",
    "    print(\"\\n--- Sampling Ranking ---\\n\")\n",
    "    print(ranking)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T00:08:16.960908Z",
     "start_time": "2025-06-10T00:05:14.411294Z"
    }
   },
   "id": "5aa6080fdb81e303"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jetzt verwenden wir extreme Varianten der Samplings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aac56e4705ddbf"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=0.8, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=0.0, top_p=1.0, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=0.8, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=0.8, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=0.8, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=1.0, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=1.0, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 200440 mit temp=2.0, top_p=1.0, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=0.8, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=0.0, top_p=1.0, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=0.8, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=0.8, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=0.8, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=0.8, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=1.0, freq_pen=0.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=1.0, freq_pen=0.0, pres_pen=2.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=1.0, freq_pen=2.0, pres_pen=0.0 ...\n",
      "‚è≥ Anfrage f√ºr Produkt 203240 mit temp=2.0, top_p=1.0, freq_pen=2.0, pres_pen=2.0 ...\n",
      "‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_maximum_sampling.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Client setup\n",
    "client = OpenAI(\n",
    "    api_key=\"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\", \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Sampling-Parameter: nur zwei Werte jeweils f√ºr extremes Vergleichssetup\n",
    "TEMPERATURES = [0.0, 2.0]       # niedrig vs. hoch\n",
    "TOP_P_VALUES = [0.8, 1.0]       # restriktiv vs. offen\n",
    "FREQUENCY_PENALTIES = [0.0, 2.0]\n",
    "PRESENCE_PENALTIES = [0.0, 2.0]\n",
    "\n",
    "MODEL_ID = \"openai/gpt-4\"\n",
    "\n",
    "def create_product_description(group):\n",
    "    lines = []\n",
    "    for _, row in group.iterrows():\n",
    "        line = f\"{row['PropertyDefinition']}: {row['PropertyValue']}\"\n",
    "        if pd.notnull(row['UnitName']):\n",
    "            line += f\" {row['UnitName']}\"\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def generate_prompt_neutral(group):\n",
    "    product_text = create_product_description(group)\n",
    "    return f\"\"\"\n",
    "Du bist ein Produktexperte.\n",
    "\n",
    "Hier ist die Beschreibung eines Produkts:\n",
    "{product_text}\n",
    "\n",
    "Frage: Welche technischen Eigenschaften oder Funktionen hat dieses Produkt? Bitte antworte m√∂glichst pr√§zise und in maximal 3 S√§tzen.\n",
    "\n",
    "ANTWORT:\n",
    "\"\"\"\n",
    "\n",
    "def run_prompt(prompt, temperature, top_p, frequency_penalty, presence_penalty):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    df = pd.read_parquet(\"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/241212_processedproperties_fhnw.parquet\")\n",
    "    grouped = df.groupby(\"ProductId\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Nur die ersten 2 Produkte f√ºr schnellen Test, kann erweitert werden\n",
    "    for product_id, group in list(grouped)[:2]:\n",
    "        prompt = generate_prompt_neutral(group)\n",
    "        row_result = {\n",
    "            \"ProductId\": product_id,\n",
    "            \"Prompt\": prompt\n",
    "        }\n",
    "\n",
    "        for temp in TEMPERATURES:\n",
    "            for top_p in TOP_P_VALUES:\n",
    "                for freq_penalty in FREQUENCY_PENALTIES:\n",
    "                    for pres_penalty in PRESENCE_PENALTIES:\n",
    "                        try:\n",
    "                            print(f\"‚è≥ Anfrage f√ºr Produkt {product_id} mit temp={temp}, top_p={top_p}, freq_pen={freq_penalty}, pres_pen={pres_penalty} ...\")\n",
    "                            output = run_prompt(prompt, temp, top_p, freq_penalty, pres_penalty)\n",
    "                            key = f\"output_t{temp}_p{top_p}_f{freq_penalty}_pr{pres_penalty}\"\n",
    "                            row_result[key] = output\n",
    "                            time.sleep(1.5)\n",
    "                        except Exception as e:\n",
    "                            key = f\"output_t{temp}_p{top_p}_f{freq_penalty}_pr{pres_penalty}\"\n",
    "                            row_result[key] = f\"Fehler: {str(e)}\"\n",
    "\n",
    "        results.append(row_result)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(\"produkt_halluzinationen_maximum_sampling.csv\", index=False)\n",
    "    print(\"‚úÖ Ergebnisse gespeichert unter 'produkt_halluzinationen_maximum_sampling.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:09:07.099209Z",
     "start_time": "2025-06-13T14:03:17.965080Z"
    }
   },
   "id": "11dacb1a4b4ea978"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Spalten im CSV: ['ProductId', 'Prompt', 'output_t0.0_p0.8_f0.0_pr0.0', 'output_t0.0_p0.8_f0.0_pr2.0', 'output_t0.0_p0.8_f2.0_pr0.0', 'output_t0.0_p0.8_f2.0_pr2.0', 'output_t0.0_p1.0_f0.0_pr0.0', 'output_t0.0_p1.0_f0.0_pr2.0', 'output_t0.0_p1.0_f2.0_pr0.0', 'output_t0.0_p1.0_f2.0_pr2.0', 'output_t2.0_p0.8_f0.0_pr0.0', 'output_t2.0_p0.8_f0.0_pr2.0', 'output_t2.0_p0.8_f2.0_pr0.0', 'output_t2.0_p0.8_f2.0_pr2.0', 'output_t2.0_p1.0_f0.0_pr0.0', 'output_t2.0_p1.0_f0.0_pr2.0', 'output_t2.0_p1.0_f2.0_pr0.0', 'output_t2.0_p1.0_f2.0_pr2.0']\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p0.8_f2.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t0.0_p1.0_f2.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p0.8_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p0.8_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p0.8_f2.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p1.0_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p1.0_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 200440 mit Sampling output_t2.0_p1.0_f2.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p0.8_f2.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t0.0_p1.0_f2.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p0.8_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p0.8_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p0.8_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p0.8_f2.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p1.0_f0.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p1.0_f0.0_pr2.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p1.0_f2.0_pr0.0...\n",
      "üîç Pr√ºfe Produkt 203240 mit Sampling output_t2.0_p1.0_f2.0_pr2.0...\n",
      "‚úÖ Faktenbewertungen gespeichert in 'gpt4_factcheck_bewertung_maximum.csv'.\n",
      "‚è≥ Generiere Sampling-Ranking mit LLM...\n",
      "\n",
      "--- Sampling Ranking ---\n",
      "\n",
      "1. Sampling: output_t0.0_p0.8_f0.0_pr0.0\n",
      "2. Sampling: output_t0.0_p0.8_f0.0_pr2.0\n",
      "3. Sampling: output_t0.0_p0.8_f2.0_pr0.0\n",
      "4. Sampling: output_t0.0_p0.8_f2.0_pr2.0\n",
      "5. Sampling: output_t0.0_p1.0_f0.0_pr0.0\n",
      "6. Sampling: output_t0.0_p1.0_f0.0_pr2.0\n",
      "7. Sampling: output_t0.0_p1.0_f2.0_pr\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Prompt zur Faktenpr√ºfung\n",
    "def generate_factcheck_prompt(product_facts, model_output):\n",
    "    return f\"\"\"\n",
    "Du bist ein strenger Faktenpr√ºfer im Bereich E-Commerce. Deine Aufgabe ist es, jede Aussage in der Produktzusammenfassung **sehr genau** mit den gegebenen Produktinformationen zu vergleichen.\n",
    "\n",
    "Bitte kategorisiere die Zusammenfassung in eine der folgenden Kategorien:\n",
    "\n",
    "1. **Faktisch korrekt** ‚Äì Alle Aussagen stimmen exakt mit den Produktinformationen √ºberein.\n",
    "2. **Teilweise korrekt** ‚Äì Einige Aussagen sind korrekt, aber es gibt auch Fehler oder √úbertreibungen.\n",
    "3. **Halluzination** ‚Äì Die Zusammenfassung enth√§lt falsche, erfundene oder nicht belegbare Informationen.\n",
    "\n",
    "Wichtig: Sei kritisch und z√∂gere nicht, 'Halluzination' zu vergeben, wenn eine Behauptung nicht klar durch die Produktinformationen gedeckt ist.\n",
    "\n",
    "---\n",
    "\n",
    "### Produktinformation:\n",
    "{product_facts}\n",
    "\n",
    "### Zusammenfassung:\n",
    "{model_output}\n",
    "\n",
    "### Deine Bewertung (nur eine Kategorie als Antwort, keine Erkl√§rungen):\n",
    "\"\"\"\n",
    "\n",
    "# GPT-4 als Judge\n",
    "def run_gpt4_factcheck(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=20\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Prompt zur Auswertung aller Ergebnisse\n",
    "def generate_ranking_prompt(results):\n",
    "    text = \"Bewerte folgendes Sampling-Setup-Ranking basierend auf der H√§ufigkeit von 'Faktisch korrekt', 'Teilweise korrekt' und 'Halluzination' Bewertungen. Gib eine Rangliste der besten Sampling-Parameter zur√ºck, die m√∂glichst wenige Halluzinationen enthalten und gute Faktentreue aufweisen.\\n\\n\"\n",
    "    for r in results:\n",
    "        text += f\"Sampling: {r['Sampling']}, Bewertung: {r['GPT-4_Faktenbewertung']}\\n\"\n",
    "    text += \"\\nBitte gib ein Ranking in folgender Form zur√ºck:\\n1. Sampling: ...\\n2. Sampling: ...\\n3. Sampling: ...\\n\\nNur das Ranking ohne weitere Erkl√§rungen.\"\n",
    "    return text\n",
    "\n",
    "# Hauptfunktion\n",
    "def main():\n",
    "    df = pd.read_csv(\n",
    "        \"/Users/ilyas/Desktop/FHNW/4. Semester/Challenge/produkt_halluzinationen_maximum_sampling.csv\", \n",
    "        sep=\";\",\n",
    "        quotechar='\"',\n",
    "        on_bad_lines='skip',\n",
    "        encoding=\"utf-8\"\n",
    "\n",
    "    )\n",
    "    print(\"üìÑ Spalten im CSV:\", df.columns.tolist())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Spalten mit Model-Output erkennen\n",
    "    output_columns = [col for col in df.columns if col.startswith(\"output_\")]\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_id = row[\"ProductId\"]\n",
    "        prompt_text = row[\"Prompt\"]\n",
    "\n",
    "        for col in output_columns:\n",
    "            model_output = row[col]\n",
    "            full_prompt = generate_factcheck_prompt(prompt_text, model_output)\n",
    "\n",
    "            try:\n",
    "                print(f\"üîç Pr√ºfe Produkt {product_id} mit Sampling {col}...\")\n",
    "                evaluation = run_gpt4_factcheck(full_prompt)\n",
    "            except Exception as e:\n",
    "                evaluation = f\"Fehler: {e}\"\n",
    "\n",
    "            results.append({\n",
    "                \"ProductId\": product_id,\n",
    "                \"Prompt\": prompt_text,\n",
    "                \"Sampling\": col,\n",
    "                \"Model_Output\": model_output,\n",
    "                \"GPT-4_Faktenbewertung\": evaluation\n",
    "            })\n",
    "\n",
    "            time.sleep(1.5)  # Rate limiting beachten\n",
    "\n",
    "    # Speichern\n",
    "    pd.DataFrame(results).to_csv(\"gpt4_factcheck_bewertung_maximum.csv\", index=False)\n",
    "    print(\"‚úÖ Faktenbewertungen gespeichert in 'gpt4_factcheck_bewertung_maximum.csv'.\")\n",
    "\n",
    "    # Ranking mit LLM generieren\n",
    "    ranking_prompt = generate_ranking_prompt(results)\n",
    "    print(\"‚è≥ Generiere Sampling-Ranking mit LLM...\")\n",
    "    ranking_response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": ranking_prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    ranking = ranking_response.choices[0].message.content.strip()\n",
    "    print(\"\\n--- Sampling Ranking ---\\n\")\n",
    "    print(ranking)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:14:09.246985Z",
     "start_time": "2025-06-13T14:12:44.142366Z"
    }
   },
   "id": "46a55335e07ad2bf"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Generiere Sampling-Ranking mit LLM...\n",
      "\n",
      "--- Sampling Ranking ---\n",
      "\n",
      "1. Sampling: output_t0.0_p0.8_f0.0_pr0.0\n",
      "2. Sampling: output_t0.0_p0.8_f0.0_pr2.0\n",
      "3. Sampling: output_t0.0_p0.8_f2.0_pr0.0\n",
      "4. Sampling: output_t0.0_p0.8_f2.0_pr2.0\n",
      "5. Sampling: output_t0.0_p1.0_f0.0_pr0.0\n",
      "6. Sampling: output_t0.0_p1.0_f0.0_pr2.0\n",
      "7. Sampling: output_t0.0_p1.0_f2.0_pr\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setup OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  \n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Prompt zur Auswertung aller Ergebnisse\n",
    "def generate_ranking_prompt(results):\n",
    "    text = \"Bewerte folgendes Sampling-Setup-Ranking basierend auf der H√§ufigkeit von 'Faktisch korrekt', 'Teilweise korrekt' und 'Halluzination' Bewertungen. Gib eine Rangliste der besten Sampling-Parameter zur√ºck, die m√∂glichst wenige Halluzinationen enthalten und gute Faktentreue aufweisen.\\n\\n\"\n",
    "    for r in results:\n",
    "        text += f\"Sampling: {r['Sampling']}, Bewertung: {r['GPT-4_Faktenbewertung']}\\n\"\n",
    "    text += \"\\nBitte gib ein Ranking in folgender Form zur√ºck:\\n1. Sampling: ...\\n2. Sampling: ...\\n3. Sampling: ...\\n\\nNur das Ranking ohne weitere Erkl√§rungen.\"\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    # Lade die bereits durchgef√ºhrte GPT-4-Faktenbewertung\n",
    "    df = pd.read_csv(\"gpt4_factcheck_bewertung_maximum.csv\")\n",
    "\n",
    "    # Daten f√ºr das Ranking vorbereiten\n",
    "    results = df[[\"Sampling\", \"GPT-4_Faktenbewertung\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    # Prompt f√ºr das Ranking generieren\n",
    "    ranking_prompt = generate_ranking_prompt(results)\n",
    "    print(\"‚è≥ Generiere Sampling-Ranking mit LLM...\")\n",
    "\n",
    "    # GPT-4 Bewertung (LLM als Meta-Judge)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": ranking_prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=150\n",
    "    )\n",
    "\n",
    "    ranking = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Ergebnis anzeigen\n",
    "    print(\"\\n--- Sampling Ranking ---\\n\")\n",
    "    print(ranking)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-13T14:26:11.886538Z",
     "start_time": "2025-06-13T14:26:03.621420Z"
    }
   },
   "id": "c27455ce027bf4df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae33d2ea69fbb8cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
